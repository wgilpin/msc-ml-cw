{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coursework\n",
    "\n",
    " Machine Learning- COIY065H7  \n",
    " William Gilpin  \n",
    " wgilpi01@dcs.bbk.ac.uk  \n",
    " wgilpin@gmail.com  \n",
    " \n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This task is undertaken as courework for the Birkbeck MSc Data Analytics pathway.\n",
    "The task is to assess the performance of the WAME optimiser [Mosca 2017],\n",
    "\"which involve adapting a different\n",
    "learning rate for each weight rather than using a single, global, learning\n",
    "rate for the entire network, we are able to reach close to state–of–the–art\n",
    "performance on the same architectures, and improve the training time and\n",
    "accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology and Design\n",
    "\n",
    "The approach is to compare WAME with 3 other optimisation methods, as originally referennced in the WAME paper [Mosca 2017].\n",
    "These are\n",
    "\n",
    "- Stochastic Gradient Descent (SGD) with momentum\n",
    "- RMSprop [Tieleman 2012]\n",
    "- Adam [Kingma 2014]\n",
    "\n",
    "### WAME Optimizer\n",
    "\n",
    "WAME (weight-wise adaptive learning rates with moving average estimator)\n",
    "modifies the learning rates *per weight* to improve training performance for a given time.\n",
    "\n",
    "As with Rprop [Riedmiller 1993], WAME takes into account the sign of the gradient and not its magnitude. In a given iteration,\n",
    "\n",
    "$\n",
    "   \\zeta_{ij}(t) =\n",
    "\\begin{cases}\n",
    "    \\min(\\zeta_{ij}(t-1)\\times \\eta_+, \\zeta_{max}),& \\text{if } \\partial\\gt 0\\\\\n",
    "    \\max(\\zeta_{ij}(t-1)\\times \\eta_+, \\zeta_{max}),& \\text{if } \\partial\\lt 0\\\\\n",
    "    \\zeta_{ij}(t-1)              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "The full algorithm also includes the following hyperparameters:\n",
    "- an exponential decay factor, $\\alpha$\n",
    "- a maximum per-weight acceleration factor for when the gradient is positive (clipping): $\\zeta_{max}$\n",
    "- a mimimum per-weight acceleration factor value for when the gradient is negative (clipping): $\\zeta_{min}$\n",
    "- an acceleration hyperparamter for when the gradient is positive: $\\eta_+$\n",
    "- an acceleration hyperparamter for when the gradient is negative: $\\eta_-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm\n",
    "\n",
    "1. **pick** $\\alpha, \\eta_+, \\eta_-, \\zeta_{min}, \\zeta_{max}$\n",
    "2. $\\theta_{ij}(0) = 0, Z_{ij} (0) = 0, \\zeta_{ij} = 1|\\forall i, j$\n",
    "3. **for all** $t \\in [1..T]$ **do**\n",
    "4. $\\quad$\n",
    "    **if** $\\frac{\\partial E(t)}{\\partial w_{ij}}\\cdot \\frac{\\partial E(t-1)}{\\partial w_{ij}}>0$\n",
    "    **then**\n",
    "5. $\\quad \\quad \\zeta_{ij}(t) =\\min(\\zeta_{ij}(t-1)\\times \\eta_+, \\zeta_{max})$\n",
    "6. $\\quad$\n",
    "    **else if** $\\frac{\\partial E(t)}{\\partial w_{ij}}\\cdot\n",
    "    \\frac{\\partial E(t-1)}{\\partial w_{ij}}>0$ **then**\n",
    "7. $\\quad \\quad \\zeta_{ij}(t) =\\max(\\zeta_{ij}(t-1)\\times \\eta_+, \\zeta_{min})$\n",
    "8. $\\quad$ **end if**\n",
    "9. $\\quad Z_{ij} (t) = \\alpha Z_{ij} (t−1) + (1 − \\alpha)\\zeta_{ij}(t)$\n",
    "10. $\\quad \\theta_{ij} (t) = \\alpha \\theta_{ij} (t−1) + (1 − \\alpha){(\\frac{\\partial E(t)}{\\partial w_{ij}}(t))}^2$\n",
    "11. $\\quad \\Delta w_{ij}(t)= -\\lambda Z_{ij}\\frac{\\partial E(t)}{\\partial w_{ij}}\\frac{1}{\\theta_{}ij(t)}$\n",
    "12. $\\quad w_{ij}(t+1) = w_{ij}(t)+\\Delta w_{ij}(t)$\n",
    "13. **end for**\n",
    "\n",
    "<div align=\"center\"><strong>WAME Algorithm</strong></div>\n",
    "\n",
    "To understand WAME it is useful to consider Rprop optimisation. Rprop deals with the fact that there is wide variation in the magnitude of the gradient on different weights. Compared to previous algorithms which used a global learning rate, Rprop maintains a per-weight learning rate. The algorithm doesn't use the magnitude of the gradietnt, rather it uses the sign of the gradient and whether the sign has changed. If the sign is unchanged between rounds then the direction of change is correct, so we accelerate by mulitplying the step size by say $\\eta_+ = 1.2$. If the sign has changed then we have overshot the optinal value, so we reduce the step size by multiplying it by say $\\eta_- = 0.1$. This helps reduce the convergence to local minima.\n",
    "\n",
    "WAME is similar to Rprop in that it applies a per weight accceleration factor $\\zeta_{ij}$, but to overcome difficulties Rprop has with large datasets and mini-batch approaches, WAME also uses the Exponential Moving Average (EMA) of $\\zeta_{ij}$ and an exponential decay factor, $\\alpha$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The dataset used is the Statlog (Landsat Satellite) Data Set [Srinivasan 1993]. This dataset contains small ($3\\times{3}$)\n",
    "multispectral images along with their classifications:\n",
    "\n",
    "1. red soil\n",
    "2. cotton crop\n",
    "3. grey soil\n",
    "4. damp grey soil\n",
    "5. soil with vegetation stubble\n",
    "6. mixture class (all types present)\n",
    "7. very damp grey soil\n",
    "\n",
    "Each record contains 3 sets of 3x3 pixel values, each set being for a different frequency, and a classification.\n",
    "\n",
    "The raw data allows 7 classes, but only 6 have samples, so the missign class is removed leaving 6 classes.\n",
    "\n",
    "The data is loaded and the classes converted into 1-hot encoding for training the multinomial logistic regression. 1-hot encoding is the translation od a set of categories into a matrix where for a given row only a single column value is set to 1, all the others being zero. This way a column represents the class in the original data.\n",
    "\n",
    "```\n",
    "classes = [A,C,B,B,C,A]\n",
    "\n",
    "one_hot_classes = A | B | C\n",
    "                  ---------\n",
    "                  1   0   0\n",
    "                  0   0   1\n",
    "                  0   1   0\n",
    "                  0   1   0\n",
    "                  0   0   1\n",
    "                  1   0   0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1 Model\n",
    "\n",
    "#### 1.1 Approach\n",
    "The problem is treated as a multinomial logistic regression, assigning one of 6 possible output classes to\n",
    "each sample.\n",
    "\n",
    "The model trained is a dense sequential ANN using Keras. The model structure has three hidden dense layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Layer (type)      |           Output Shape   |           Param # | Activation\n",
    " -----------------|----------------------------|----------------|----\n",
    " input_1 (InputLayer)      | (None, 36)    |            0     |\n",
    "dense_1 (Dense)         |   (None, 64)       |         2368  | Relu\n",
    "dense_2 (Dense)         |   (None, 128)      |         8320  | Relu\n",
    "dropout_1 (Dropout)     |   (None, 128)      |         0 |\n",
    "dense_3 (Dense)         |   (None, 64)       |         8256 | sigmoid\n",
    "dense_4 (Dense)         |   (None, 6)        |         390  | softmax\n",
    "\n",
    "Total params: 19,334\n",
    "Trainable params: 19,334\n",
    "Non-trainable params: 0\n",
    "\n",
    "The loss function is Keras' built-in `categorical_crossentropy` and the optimizer varies per experiment.\n",
    "\n",
    "This model structure was iterated over the following parameters to determine optimal performance:\n",
    "- Number of hidden layers: 1-3\n",
    "- Units per layer: 20-128\n",
    "- Dropout layers: 0-2\n",
    "- Dropout fraction: 0.1-0.5\n",
    "- Dense activation functions: relu/sigmoid\n",
    "\n",
    "Various tuning functions were created to iterate over these parameters, as well as hand tuning.\n",
    "\n",
    "The final layer softmax selects the model class with the highest probability as the chosen class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Callbacks*\n",
    "\n",
    "Two keras callbacks were created:\n",
    "- a CSV logger which logs loss for each epoch duting training\n",
    "- a custom timer callback which records the time at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Techniques and parameters\n",
    "The loss function is Keras' built-in `categorical_crossentropy` and the optimizer varies per experiment.\n",
    "\n",
    "This model structure was iterated over the following parameters to determine optimal performance:\n",
    "- Number of hidden layers: 1-3\n",
    "- Units per layer: 20-128\n",
    "- Dropout layers: 0-2\n",
    "- Dropout fraction: 0.1-0.5\n",
    "- Dense activation functions: relu/sigmoid\n",
    "\n",
    "Various tuning functions were created to iterate over these parameters, as well as hand tuning.\n",
    "\n",
    "*Regularisation*\n",
    "\n",
    "L2 regularisation was used to reduce the risk of overfitting. Without regularisation\n",
    "a model penalises for error (loss), but in order to minimise overfitting the regularised model will\n",
    "also penalise for model complexity. The final model has a large number of units per layer, and regularisation\n",
    "improved performance.\n",
    "\n",
    "Without Regularisation  \n",
    "$\\quad minimise(Error(y,\\hat{y}))$\n",
    "\n",
    "With Regularisation  \n",
    "$\\quad minimise(Error(y,\\hat{y}) + Complexity(model))$\n",
    "\n",
    "L2 regularisation allows for the complexity by adding a factor to the error based on the sum of the\n",
    "squares of the weights in the model, on the understanding that a lot of larger-magnitude weights\n",
    "probably implies a complex model.\n",
    "\n",
    "L2 regularisation  \n",
    "$\\quad minimise(Error(y,\\hat{y}) + \\lambda\\sum_{i}{w_i}^2)$\n",
    "\n",
    "*Parameters*\n",
    "\n",
    "The experiment started with default parameters for all models. Refinement led to the following choices:\n",
    "\n",
    "WAME: Epsilon = 0.01 (tested $10^{-1} \\rightarrow 10^{-5}$)  \n",
    "Adam: Learning rate = 0.01  \n",
    "RMSprop: Learning rate=0.01\n",
    "\n",
    "Epochs: 300 (tested 20-300)\n",
    "\n",
    "Batch Size: 64 (tested 40-128)\n",
    "\n",
    "L2 regulariser: Lambda=0.01 (tested 0.1, 0.01, 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental approach followed these steps:\n",
    "\n",
    "- Data pre-processing\n",
    "- Model iterations deriving parameters & structure as above\n",
    "- Model execution per optimiser collecting:\n",
    "  - execution times per epoch\n",
    "  - scores per 60 epochs\n",
    "  \n",
    "#### 2.1 Findings\n",
    "  \n",
    "*Loss*\n",
    "\n",
    "Adam performed well, with minimum loss of 0.866 and predictable improvement as the numbert of iterations increased.\n",
    "![Adam Scores](figures/Adam-score.png)\n",
    "\n",
    "SGD was less predictable. Its lowest loss was 0.581 after 180 epochs, but the loss then increased suggesting overfitting after 180 epochs.\n",
    "\n",
    "![SGD Scores](figures/SGD-score.png)\n",
    "\n",
    "RMSprop minimum loss was 1.775 after 300 iterations, and apparently still improving.\n",
    "\n",
    "![RMSprop Scores](figures/RMSprop-score.png)\n",
    "\n",
    "WAME performed worse than the other 3 optimisers, achieving a minimum loss of 3.180 after 300 epochs. Although loss was still reducing at this point, the loss is till much higher than other methods.\n",
    "\n",
    "![WAME Scores](figures/WAME-score.png)\n",
    "\n",
    "*Times*\n",
    "\n",
    "Suprisingly, WAME was the slowest method (see Conclusions), performing slower than the others under all conditions.\n",
    "\n",
    "![Window](figures/all-optimizers-window.png \"Time over a slding window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The times for different numbers of iterations are as follows:\n",
    "    \n",
    "N          |Adam       |WAME        |SGD    |RMSprop\n",
    "----|------------|-----------|-----------|---------\n",
    "60  | 10.772892  |11.859741  |10.785231  |10.219459\n",
    "120  |19.247254  |24.450373  |18.957702  |19.192914\n",
    "180  |28.320045  |34.688663  |27.547161  |28.534963\n",
    "240  |41.103819  |46.036945  |36.807399  |37.486725\n",
    "300  |53.155509  |57.753125  |46.042100  |46.922082\n",
    "\n",
    "![Durations](figures/all-durations.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Discussion\n",
    "\n",
    "WAME is designed to \"increased learning speed without\n",
    "compromising generalisation performance.\" [Mosca 2017]. The results presented here do not support that conclusion, leading to doubt about the implementation presented here.\n",
    "\n",
    "Overall WAME scored worse, and performed slower, than the other methods assessed in all experimental conditions given the model design and the dataset used.\n",
    "\n",
    "Possible reasons\n",
    "\n",
    "1. WAME performs badly. In the balance of probabilities the peer-reviewed Mosca paper is more likely to reflect a well implemented algorithm than this implementation, although the lack of access to the Mosca source code makes direct comparison hard.\n",
    "2. WAME is inappropriate to the problem presented. Although the data is a series of images, those images are themselves 3x3 pixels, which did not allow the use of a convolutional network.\n",
    "3. The model structure used does not take advantage of WAME.\n",
    "4. Coding errors. The code can be found at [https://github.com/wgilpin/msc-ml-cw]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "\n",
    "#### 3.1 Summary\n",
    "\n",
    "Overall the recommendation must be not to adopt WAME, given the results of this analysis. The second recommendation is to improve the analysis, perhaps running WAME against the datasets and models described in the [Mosca 2017] to confirm the approach.\n",
    "\n",
    "#### 3.2 Areas for improvement\n",
    "\n",
    "1. Validation of this WAME implementation: [Mosca 2017] evaluates WAME against MNIST & CIFAR datasets, utilising CNNs. Test this implementation against the same data using the same models.\n",
    "2. Consider adding early-stopping. Although the model as run did not converge, with more epochs it should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bibliography\n",
    "\n",
    "Mosca, A., & Magoulas, G. D. (2017, April).\n",
    "  Training convolutional networks with weight-wise adaptive learning rates. In ESANN.\n",
    "\n",
    "Kingma, D. P., & Ba, J. (2014).\n",
    "  Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n",
    "\n",
    "Riedmiller, M., & Braun, H. (1993, March).\n",
    " A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In IEEE international conference on neural networks (pp. 586-591). IEEE.\n",
    "\n",
    "Srinivasan, A. (1993, February).\n",
    "  UCI Machine Learning Repository [http://archive.ics.uci.edu/ml/datasets/Statlog+(Landsat+Satellite)].\n",
    "  Glasgow, UK: University of Strathclyde, Department of Statistics and Data Modeling.\n",
    "\n",
    "Tieleman, T. & Hinton, G. (2012) Lecture 6.5—RmsProp: Divide the gradient by\n",
    "a running average of its recent magnitude. COURSERA: Neural Networks for\n",
    "Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Academic Declaration\n",
    "\n",
    "The following sources were adapted for use in this experiment:\n",
    "\n",
    "Landsat data extraction: https://github.com/abarthakur/trepan_python/blob/master/run.py\n",
    "\n",
    "WAME algorithm implementation: https://github.com/nitbix/keras-oldfork/blob/master/keras/optimizers.py\n",
    "\n",
    "Timer callback: from https://stackoverflow.com/a/43186440\n",
    "\n",
    "I acknowledge that I have read and understood the sections on plagiarism in the College Policy on \n",
    "assessment offences and confirm that the work is my own, with the work of others clearly \n",
    "acknowledged. I also give my permission to submit my files to the plagiarism testing database \n",
    "that the College is using and test it using plagiarism detection software (Turnitin), search \n",
    "engines or meta-searching software.\n",
    "\n",
    "<img src=\"img/signature.jpg\" width=\"150\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
